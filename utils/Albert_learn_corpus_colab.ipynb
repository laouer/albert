{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Albert_learn_corpus_tutorial",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8SJfpgTccDB"
      },
      "source": [
        "\n",
        "<a href=\"https://colab.research.google.com/github/google-research/albert/blob/master/albert_glue_fine_tuning_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHQH4OCHZ9bq",
        "cellView": "form"
      },
      "source": [
        "# @title Copyright 2020 The ALBERT Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkTLZ3I4_7c_"
      },
      "source": [
        "# ALBERT End to End (Fine-tuning + Predicting) with Cloud TPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wtjs1QDb3DX"
      },
      "source": [
        "## Overview\n",
        "\n",
        "ALBERT is \"A Lite\" version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter-reduction techniques that allow for large-scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation.\n",
        "\n",
        "For a technical description of the algorithm, see our paper:\n",
        "\n",
        "https://arxiv.org/abs/1909.11942\n",
        "\n",
        "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut\n",
        "\n",
        "This Colab demonstates using a free Colab Cloud TPU to fine-tune GLUE tasks built on top of pretrained ALBERT models and \n",
        "run predictions on tuned model. The colab demonsrates loading pretrained ALBERT models from both [TF Hub](https://www.tensorflow.org/hub) and checkpoints.\n",
        "\n",
        "**Note:**  You will need a GCP (Google Compute Engine) account and a GCS (Google Cloud \n",
        "Storage) bucket for this Colab to run.\n",
        "\n",
        "Please follow the [Google Cloud TPU quickstart](https://cloud.google.com/tpu/docs/quickstart) for how to create GCP account and GCS bucket. You have [$300 free credit](https://cloud.google.com/free/) to get started with any GCP product. You can learn more about Cloud TPU at https://cloud.google.com/tpu/docs.\n",
        "\n",
        "This notebook is hosted on GitHub. To view it in its original repository, after opening the notebook, select **File > View on GitHub**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld-JXlueIuPH"
      },
      "source": [
        "## Instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POkof5uHaQ_c"
      },
      "source": [
        "<h3><a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>  &nbsp;&nbsp;Train on TPU</h3>\n",
        "\n",
        "   1. Create a Cloud Storage bucket for your TensorBoard logs at http://console.cloud.google.com/storage and fill in the BUCKET parameter in the \"Parameters\" section below.\n",
        " \n",
        "   1. On the main menu, click Runtime and select **Change runtime type**. Set \"TPU\" as the hardware accelerator.\n",
        "   1. Click Runtime again and select **Runtime > Run All** (Watch out: the \"Colab-only auth for this notebook and the TPU\" cell requires user input). You can also run the cells manually with Shift-ENTER."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdMmwCJFaT8F"
      },
      "source": [
        "### Set up your TPU environment\n",
        "\n",
        "In this section, you perform the following tasks:\n",
        "\n",
        "*   Set up a Colab TPU running environment\n",
        "*   Verify that you are connected to a TPU device\n",
        "*   Upload your credentials to TPU to access your GCS bucket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "191zq3ZErihP"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import os\n",
        "import pprint\n",
        "import json\n",
        "import tensorflow as tf\n",
        "%load_ext tensorboard\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "if \"COLAB_TPU_ADDR\" in os.environ :\n",
        "  # assert \"COLAB_TPU_ADDR\" in os.environ, \"ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!\"\n",
        "  TPU_ADDRESS = \"grpc://\" + os.environ[\"COLAB_TPU_ADDR\"] \n",
        "  TPU_TOPOLOGY = \"2x2\"\n",
        "  print(\"TPU address is\", TPU_ADDRESS)\n",
        "\n",
        "  with tf.Session(TPU_ADDRESS) as session:\n",
        "    print('TPU devices:')\n",
        "    pprint.pprint(session.list_devices())\n",
        "\n",
        "    # Upload credentials to TPU.\n",
        "    with open('/content/adc.json', 'r') as f:\n",
        "      auth_info = json.load(f)\n",
        "    tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUBP35oCDmbF"
      },
      "source": [
        "### Prepare and import ALBERT modules\n",
        "â€‹\n",
        "With your environment configured, you can now prepare and import the ALBERT modules. The following step clones the source code from GitHub."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wzwke0sxS6W",
        "cellView": "code"
      },
      "source": [
        "#TODO(lanzhzh): Add pip support\n",
        "import sys\n",
        "!rm -rf sample_data\n",
        "!test -d albert || git clone https://github.com/google-research/albert albert\n",
        "if not 'albert' in sys.path:\n",
        "  sys.path += ['albert']\n",
        "\n",
        "!test -d albert-utils || git clone https://github.com/laouer/albert-utils.git albert-utils\n",
        "\n",
        "!pip install sentencepiece progressbar\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRu1aKO1D7-Z"
      },
      "source": [
        "### Prepare for training\n",
        "This next section of code performs the following tasks:\n",
        "*  Specify GS bucket, create output directory for model checkpoints and eval results.\n",
        "*  Make sure that the Input files are located on the GS bucket\n",
        "*  Specify ALBERT pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKpQ6D1-pjTH"
      },
      "source": [
        "BUCKET = \"\" #@param { type: \"string\" }\n",
        "# Available pretrained model checkpoints:\n",
        "#   base, large, xlarge, xxlarge\n",
        "ALBERT_MODEL = \"base\" #@param [\"base\", \"large\",\"xlarge\",\"xxlarge\"]\n",
        "\n",
        "BASE_DIR = \"gs://\" + BUCKET\n",
        "if not BASE_DIR or BASE_DIR == \"gs://\":\n",
        "  raise ValueError(\"You must enter a BUCKET.\")\n",
        "\n",
        "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
        "OUTPUT_DIR = 'gs://{}/albert-tfhub/models/WIKIFR/{}'.format(BUCKET, ALBERT_MODEL)\n",
        "FEATURE_DIR = 'gs://{}/albert-tfhub/models/WIKIFR/wikifr_tf_records/'.format(BUCKET)\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n",
        "\n",
        "ALBERT_MODEL_HUB_NAME=\"albert_\"+ ALBERT_MODEL + \"_3\"\n",
        "ALBERT_MODEL_HUB = 'https://tfhub.dev/google/albert_' + ALBERT_MODEL + '/3'\n",
        "\n",
        "!wget -q --content-disposition  $ALBERT_MODEL_HUB\"?tf-hub-format=compressed\" && \\\n",
        "      mkdir -p $ALBERT_MODEL_HUB_NAME && \\\n",
        "      tar -xf $ALBERT_MODEL_HUB_NAME\".tar.gz\" -C $ALBERT_MODEL_HUB_NAME && \\\n",
        "      rm -rf $ALBERT_MODEL_HUB_NAME\".tar.gz\"\n",
        "\n",
        "ALBERT_CONFIG_FILE = ALBERT_MODEL_HUB_NAME + \"/assets/albert_config.json\"\n",
        "INIT_CHECKPOINT_FILE=ALBERT_MODEL_HUB_NAME + \"/variables/\"\n",
        "INPUT_FILES_PATTERN=FEATURE_DIR + \"*/*.tf_record\"\n",
        "\n",
        "print('***** Input files pattern: {} *****'.format(INPUT_FILES_PATTERN))\n",
        "\n",
        "%tensorboard --logdir $OUTPUT_DIR\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnmI3vWVc8xA"
      },
      "source": [
        "# max_seq_length and max_predictions_per_seq must be the same as use for create_pretraining_data\n",
        "\n",
        "max_seq_length=128\n",
        "masked_lm_prob=0.2\n",
        "max_predictions_per_seq=int(masked_lm_prob*max_seq_length)\n",
        "\n",
        "!python -m albert.run_pretraining \\\n",
        "  --albert_config_file=$ALBERT_CONFIG_FILE \\\n",
        "  --input_file=$INPUT_FILES_PATTERN \\\n",
        "  --output_dir=$OUTPUT_DIR\\\n",
        "  --export_dir=$OUTPUT_DIR\"/export/\"\\\n",
        "  --max_seq_length=$max_seq_length \\\n",
        "  --max_predictions_per_seq=$max_predictions_per_seq \\\n",
        "  --train_batch_size=1024 \\\n",
        "  --num_train_steps=200000 \\\n",
        "  --do_eval=True \\\n",
        "  --tpu_name=$TPU_ADDRESS \\\n",
        "  --use_tpu=True "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}